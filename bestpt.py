import argparse, json, cv2, torch, yaml, shutil, itertools
from pathlib import Path
from typing import List
import pandas as pd
import wandb
from sklearn.model_selection import train_test_split

from torch.utils.data import Dataset, DataLoader, Subset
from pororo.models.brainOCR.recognition import get_recognizer
from pororo.models.brainOCR import brainocr      # Reader í´ë˜ìŠ¤
from pororo.tasks import download_or_load
from pororo.models.brainOCR.brainocr import Reader   # Reader ì•ˆì— ì´ë¯¸ util ì¡´ì¬
from datasets import _BaseCrops, collate_eval, collate_train, evaluate_dataset, recognize_imgs1, recognize_imgs2
import torch.nn.functional as F
import re
import numpy as np
import os

def get_unique_save_dir(base_dir="assets", prefix="test"):
    os.makedirs(base_dir, exist_ok=True)  # assets í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±

    idx = 1
    while True:
        save_dir = os.path.join(base_dir, f"{prefix}_{idx}")
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            return save_dir
        idx += 1

FORBIDDEN = re.compile(r'[â†â†’â†”â†•â†–â†—â†˜â†™â”âœÂ·â—ã€‘â‰ â—‹â†‘Ã—â– â–¡â–²â–³â–¼â–½â—‡â—†â˜…]')  # ê²½ê³ ì— ë‚˜ì˜¨ íŠ¹ìˆ˜ ë¬¸ìë“¤ì„ ì¶”ê°€
tbl = str.maketrans({"\n": " ", "\t": " "})  # ë¹ ë¥¸ ì¹˜í™˜ìš© table
UNKNOWN_SET = set() 

# --------------------------------------------------------------------------
# 1. ë°ì´í„°ì…‹ + collate
# --------------------------------------------------------------------------
class HandwritingCrops(Dataset):
    """
    root/
      â”œâ”€ label/xxxx_metadata.json
      â”œâ”€ *.png ...
    """

    def __init__(self,
                 root_dir: str,
                 converter,
                 use_corrected: bool = True,
                 img_size: tuple = (100, 64)) -> None:
        self.root = Path(root_dir)
        self.imgW, self.imgH = img_size
        self.converter = converter
        self.samples = []  # [(png_path, text)]

        df = pd.read_csv(self.root / "merged_labels.csv", dtype={"text": str})
        for _, row in df.iterrows():
            img_path = self.root / "merged_images" / row["filename"]
            txt = row["text"]
            if not isinstance(txt, str) or not txt.strip():
                continue
            self.samples.append((img_path, txt))

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        png_fp, label = self.samples[idx]

        # ---------- ì´ë¯¸ì§€ ----------
        img = cv2.imread(str(png_fp), cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, (self.imgW, self.imgH), interpolation=cv2.INTER_AREA)
        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.

        # ---------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ----------
        cleaned = (label.translate(tbl)            # \n, \t â†’ space
                         .strip())
        cleaned = re.sub(FORBIDDEN, " ", cleaned)
        cleaned = re.sub(r"\s{2,}", " ", cleaned)

        # ---------- í—ˆìš© ê¸€ì ì²´í¬ ----------
        if any(ch not in self.converter.char2idx for ch in cleaned):
            # â€» ì „ì—­ìœ¼ë¡œ ëª¨ì•„ ë‘ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì—  UNKNOWN_SET.update(...)
            return None     # â† DataLoaderì—ì„œ ë²„ë ¤ì§ˆ ìƒ˜í”Œ

        text, length = self.converter.encode([cleaned])
        
        max_t = self.imgW // 4           # down-sampling 1/4 ê°€ì •
        if length > max_t:
            return None 
        return img, text, length


# --------------------------------------------------------------------------
# 2. recognizer ë¡œë” (opt.txt â†’ dict â†’ get_recognizer)
# --------------------------------------------------------------------------
def load_opt(opt_txt: str) -> dict:
    opt2val = {}
    for ln in Path(opt_txt).read_text(encoding="utf-8").splitlines():
        if ": " not in ln:
            continue
        k, v = ln.split(": ", 1)
        try:
            opt2val[k] = yaml.safe_load(v)         # ìˆ«ìÂ·bool ë„ í˜• ë³€í™˜
        except Exception:
            opt2val[k] = v
    return opt2val


def build_recognizer(opt_txt_fp: str, device: str = "cuda"):
    # 1) ì˜µì…˜ txt ì½ê¸°
    opt = Reader.parse_options(opt_txt_fp)

    # 2) vocab ì„¸íŒ…
    opt["vocab"] = Reader.build_vocab(opt["character"])
    opt["vocab_size"] = len(opt["vocab"])
    opt["num_class"] = opt["vocab_size"]

    # 3) ëª¨ë¸ ckpt ê²½ë¡œ ì§€ì •  (â† ë¹ ì ¸ ìˆì–´ì„œ KeyError ë°œìƒ)
    default_ckpt = "brainocr.pt"
    opt["rec_model_ckpt_fp"] = str(default_ckpt)

    # 4) ê¸°íƒ€
    opt["device"] = device
    # opt["imgH"], opt["imgW"] = 64, 256

    # 5) recognizer, converter ë°˜í™˜
    model, converter = get_recognizer(opt)
    model.to(device)
    
    # ---------- [NEW] : decode() ê°€ ì—†ìœ¼ë©´ decode_greedy ë¡œ ë˜í•‘ ----------
    if not hasattr(converter, "decode"):
        import types
        def _decode(self, flat_idx, len_tensor):
            return self.decode_greedy(flat_idx, len_tensor)
        converter.decode = types.MethodType(_decode, converter)
    # ----------------------------------------------------------------------
    return model, converter, opt

def recursive_train(model):
    model.train()
    for child in model.children():
        recursive_train(child)

def clean_text(text: str) -> str:
    # evaluate_dataset ë‚´ë¶€ì—ì„œ ì“°ì´ëŠ” í´ë¦° í•¨ìˆ˜ì™€ ë™ì¼ ë¡œì§
    return re.sub(r"[^a-zA-Z0-9ê°€-í£]", "", text).lower()

def evaluate_accuracy(recognizer, op2val, converter, test_loader, device="cuda"):
    """
    evaluate_datasetê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ë³„ GTâˆ™PRì„ ë¹„êµí•˜ì—¬ 
    'ê³µë°± ë¬´ì‹œ + ì†Œë¬¸ì + í•œê¸€Â·ì˜ì–´Â·ìˆ«ìë§Œ ë‚¨ê¹€' ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    recognizer.eval()
    hit_cleaned = 0
    total = 0
    total_loss = 0.0
    criterion = torch.nn.CTCLoss(zero_infinity=True)
    
    sum_counts = [0, 0, 0]  # íŠ¹ìˆ˜ê¸°í˜¸, ìˆ«ì, ê¸°ë³¸ ê¸€ì ì¹´ìš´íŠ¸
    sum_conf_sums = [0.0, 0.0, 0.0]  # ê° ì¹´ìš´íŠ¸ì˜ confidence í•©ê³„

    # í•„ìš”í•œ opt2valì„ ë™ì ìœ¼ë¡œ ìƒì„± (imgH, imgW, batch_size, adjust_contrast)
    with torch.no_grad():
        for batch in test_loader:
            if batch is None:
                continue

            # ë°°ì¹˜ íƒ€ì… í™•ì¸
            if len(batch) == 3:  # train collate (imgs, tgt, tgt_len)
                imgs, tgt, tgt_len = batch
                imgs = imgs.to(device)

                # Loss ê³„ì‚°
                logits = recognizer(imgs)
                log_probs = logits.log_softmax(2).permute(1, 0, 2)
                input_len = torch.full(
                    (imgs.size(0),), logits.size(1),
                    dtype=torch.long, device="cpu"
                )
                loss = criterion(log_probs.cpu(), tgt.cpu(), input_len, tgt_len.cpu())
                if not (torch.isinf(loss) or torch.isnan(loss)):
                    total_loss += loss.item()

                # ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•´ GT í…ìŠ¤íŠ¸ ë””ì½”ë”©
                if hasattr(converter, "decode"):
                    gt_texts = converter.decode(tgt, tgt_len)
                else:
                    gt_texts = converter.decode_greedy(tgt, tgt_len)

                # ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ìƒì„±
                img_list = (imgs[:, 0] * 255).byte().cpu().numpy()
                img_list = [img for img in img_list]
                preds, counts, sum_confs = recognize_imgs2(img_list, recognizer, converter, op2val)

                # ë°°ì¹˜ë³„ ëª¨ë¸ í†µê³„ ëˆ„ì 
                for i in range(3):
                    sum_counts[i] += counts[i]
                    sum_conf_sums[i] += sum_confs[i]

                for (pr_txt, conf), gt in zip(preds, gt_texts):
                    gt_clean = clean_text(gt)
                    pr_clean = clean_text(pr_txt)
                    if gt_clean == pr_clean:
                        hit_cleaned += 1
                    total += 1

            else:  # eval collate (imgs, gt_texts)
                imgs, gt_texts = batch
                imgs = imgs.to(device)

                # ì˜ˆì¸¡ ìˆ˜í–‰
                img_list = (imgs[:, 0] * 255).byte().cpu().numpy()
                img_list = [img for img in img_list]
                preds, counts, sum_confs = recognize_imgs2(img_list, recognizer, converter, op2val)

                # ë°°ì¹˜ë³„ ëª¨ë¸ í†µê³„ ëˆ„ì 
                for i in range(3):
                    sum_counts[i] += counts[i]
                    sum_conf_sums[i] += sum_confs[i]

                # ì •í™•ë„ ê³„ì‚°
                for (pr_txt, conf), gt in zip(preds, gt_texts):
                    gt_clean = clean_text(gt)
                    pr_clean = clean_text(pr_txt)
                    if gt_clean == pr_clean:
                        hit_cleaned += 1
                    total += 1

    # ì „ì²´ í‰ê·  confidence ê³„ì‚°
    overall_avg_confs = [
        (sum_conf_sums[i] / sum_counts[i]) if sum_counts[i] else 0.0
        for i in range(3)
    ]

    if total == 0:
        return 0.0, 0.0

    print(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë¸ë³„ (count/avg_conf) íŠ¹ìˆ˜ê¸°í˜¸: {sum_counts[0]}/{overall_avg_confs[0]:.3f}, ìˆ«ì: {sum_counts[1]}/{overall_avg_confs[1]:.3f}, ê¸°ë³¸: {sum_counts[2]}/{overall_avg_confs[2]:.3f}")

    avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0.0
    return hit_cleaned / total, avg_loss, sum_counts, overall_avg_confs

# --------------------------------------------------------------------------
# 3. íŒŒì¸íŠœë‹ í•¨ìˆ˜
# --------------------------------------------------------------------------
def finetune(recognizer, op2val, converter, train_loader, valid_loader, test_loader, epochs, lr, save_dir: Path, device="cuda"):
    # ì†ì‹¤âˆ™ì˜µí‹°ë§ˆì´ì €âˆ™ìŠ¤ì¼€ì¤„ëŸ¬ ì„¸íŒ…
    criterion = torch.nn.CTCLoss(zero_infinity=True)
    optimizer = torch.optim.Adam(
        recognizer.parameters(), lr=lr, weight_decay=1e-5
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=5, verbose=True
    )

    recognizer.to(device)
    recognizer.train()

    best_acc = 0.0  # ìµœê³  validation ì •í™•ë„ ê¸°ë¡

    for ep in range(1, epochs + 1):
        running_loss = 0.0
        batch_count = 0

        # Training phase
        recognizer.train()
        for step, batch in enumerate(train_loader):
            if batch is None:
                continue

            imgs, tgt, tgt_len = batch
            imgs = imgs.to(device)

            recognizer.zero_grad()
            try:
                logits = recognizer(imgs)  # (B, T, C)
                log_probs = logits.log_softmax(2).permute(1, 0, 2)
                input_len = torch.full(
                    (imgs.size(0),), logits.size(1),
                    dtype=torch.long, device="cpu"
                )
                loss = criterion(log_probs.cpu(), tgt.cpu(), input_len, tgt_len.cpu())

                if torch.isinf(loss) or torch.isnan(loss):
                    print(f"[skip] ep{ep} step{step}  loss={loss.item()}")
                    continue

                loss.backward()
                torch.nn.utils.clip_grad_norm_(recognizer.parameters(), 1.0)
                optimizer.step()

                running_loss += loss.item()
                batch_count += 1
            except RuntimeError as e:
                print(f"[error skip] {e}")
                continue

        # epochë³„ í‰ê·  training loss
        avg_train_loss = running_loss / batch_count if batch_count > 0 else float("nan")
        current_lr = optimizer.param_groups[0]['lr']
        print(f"[epoch {ep}/{epochs}] train_loss={avg_train_loss:.6f}, lr={current_lr:.2e}")

        # â”€â”€ Validation phase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        val_acc, val_loss, val_sum_counts, val_avg_confs = evaluate_accuracy(recognizer, op2val, converter, valid_loader, device=device)
        print(f"--> [Validation] epoch {ep}: accuracy={val_acc*100:.2f}%, loss={val_loss:.6f}")
        
        test_acc, test_loss, test_sum_counts, test_avg_confs = evaluate_accuracy(recognizer, op2val, converter, test_loader, device=device)
        print(f"--> [Test] epoch {ep}: accuracy={test_acc*100:.2f}%, loss={test_loss:.6f}")
        
        # Learning rate scheduler step
        scheduler.step(val_loss)
        
        # WandB ë¡œê¹… (nested dictionary structure for grouping)
        wandb.log({
            "epoch": ep,
            "train": {
                "loss": avg_train_loss,
                "accuracy": val_acc,  # training accuracyëŠ” validationê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
            },
            "validation": {
                "accuracy": val_acc,
                "loss": val_loss,
                "count_model3": val_sum_counts[0],
                "count_model2": val_sum_counts[1],
                "count_model1": val_sum_counts[2],
                "avg_conf_model3": val_avg_confs[0],
                "avg_conf_model2": val_avg_confs[1],
                "avg_conf_model1": val_avg_confs[2],
            },
            "test": {
                "accuracy": test_acc,
                "loss": test_loss,
                "count_model3": test_sum_counts[0] if 'test_sum_counts' in locals() else None,
                "count_model2": test_sum_counts[1] if 'test_sum_counts' in locals() else None,
                "count_model1": test_sum_counts[2] if 'test_sum_counts' in locals() else None,
                "avg_conf_model3": test_avg_confs[0] if 'test_avg_confs' in locals() else None,
                "avg_conf_model2": test_avg_confs[1] if 'test_avg_confs' in locals() else None,
                "avg_conf_model1": test_avg_confs[2] if 'test_avg_confs' in locals() else None,
            },
            "lr": current_lr
        })

        # â”€â”€ ìµœê³  validation ì •í™•ë„ ê°±ì‹  ì‹œ best.pt ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if val_acc > best_acc:
            best_acc = val_acc
            best_fp = save_dir / "best.pt"
            torch.save(recognizer.state_dict(), best_fp)
            print(f"ğŸŸ¢ New best model saved (epoch {ep}, val_acc={val_acc*100:.2f}%) â†’ {best_fp}")

        # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ë³€í™˜
        recursive_train(recognizer)

    print(f"â˜… Training ë. Best validation accuracy={best_acc*100:.2f}%")
    return



# --------------------------------------------------------------------------
# 4. ì €ì¥ & ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸
# --------------------------------------------------------------------------
def save_ckpt(recognizer, opt_dict, save_dir: Path):
    save_dir.mkdir(parents=True, exist_ok=True)
    ckpt_fp = save_dir / "finetune_clova.pt"
    torch.save(recognizer.state_dict(), ckpt_fp)

    opt_fp = save_dir / "finetune_clova_opt.txt"
    with opt_fp.open("w", encoding="utf-8") as f:
        for k, v in opt_dict.items():
            if k == "device":         # runtime ì •ë³´ëŠ” ì œì™¸
                continue
            f.write(f"{k}: {v}\n")
    print(f"âœ“ Saved ckpt â†’ {ckpt_fp}\nâœ“ Saved opt  â†’ {opt_fp}")
    return ckpt_fp, opt_fp


def quick_eval(reader, data_loader, device="cuda", n_show=3):
    reader.recognizer.eval()

    blank, idx2char = 0, reader.converter.idx2char
    has_decode = hasattr(reader.converter, "decode")

    for batch in data_loader:
        if batch is None:
            continue

        # â”€â”€ 2-tuple ? 3-tuple ? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if len(batch) == 3:                  # (imgs, tgt_idx, tgt_len)
            imgs, tgt, tgt_len = batch
            str_gt_available = False
        else:                                # (imgs, gt_text_list)
            imgs, gt_texts = batch
            str_gt_available = True

        imgs = imgs.to(device)
        with torch.no_grad():
            logits = reader.recognizer(imgs)

        preds_idx  = logits.softmax(2).argmax(2)
        preds_size = torch.full(
            (preds_idx.size(0),), preds_idx.size(1),
            dtype=torch.long, device=preds_idx.device)

        # â”€â”€ idx â†’ text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if has_decode:
            pred_txts, _ = reader.converter.decode(preds_idx, preds_size)
        else:                               # manual greedy
            pred_txts = []
            for seq, T in zip(preds_idx, preds_size):
                prev, chs = blank, []
                for t in range(T):
                    idx = seq[t].item()
                    if idx != blank and idx != prev:
                        chs.append(idx2char[idx])
                    prev = idx
                pred_txts.append("".join(chs))

        # â”€â”€ GT í…ìŠ¤íŠ¸ ì–»ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if str_gt_available:
            gts = gt_texts
        else:
            if has_decode:
                gts, _ = reader.converter.decode(tgt, tgt_len)
            else:
                gts = reader.converter.decode_greedy(tgt, tgt_len)

        # â”€â”€ print some â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for gt, pr in zip(gts[:n_show], pred_txts[:n_show]):
            print(f"GT: {gt}\nPR: {pr}\n")
        break

def split_train_valid(dataset, val_ratio=0.2, random_state=42):
    """
    ë°ì´í„°ì…‹ì„ train/validationìœ¼ë¡œ ë¶„ë¦¬
    """
    indices = list(range(len(dataset)))
    train_indices, val_indices = train_test_split(
        indices, test_size=val_ratio, random_state=random_state
    )
    
    train_subset = Subset(dataset, train_indices)
    val_subset = Subset(dataset, val_indices)
    
    print(f"ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ: Train={len(train_subset)}, Valid={len(val_subset)}")
    return train_subset, val_subset

def train_and_evaluate(epochs, batch_size, lr, args):
    save_dir = Path(get_unique_save_dir(base_dir=args.save_dir, prefix="finetune"))
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {save_dir}")

    wandb.init(
        project="brainocr-fine-tuning",
        name=f"{save_dir.name}_ep{epochs}_lr{lr}_bs{batch_size}",
        config={
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": lr,
            "device": args.device,
            "opt_txt": args.opt_txt,
            "train_root": args.train_root,
            "test_root": args.test_root,
            "val_ratio": args.val_ratio
        }
    )

    # â‘  recognizer/ converter ìƒì„±
    rec, converter, opt_dict = build_recognizer(args.opt_txt, device=args.device)

    # â‘¡ ì „ì²´ train ì…‹ ì¤€ë¹„
    full_train_set = _BaseCrops(
        csv_fp=Path(args.train_root) / "train_labels.csv",
        img_dir=Path(args.train_root) / "merged_images",
        img_size=(256, 64),
        converter=converter,
        for_train=True,
    )
    full_train_set.preload_for_stats()
    full_train_set.print_filter_report()

    # â‘¢ train/validation ë¶„ë¦¬
    train_subset, val_subset = split_train_valid(full_train_set, val_ratio=args.val_ratio)

    # â‘£ test ì…‹ ì¤€ë¹„
    test_set = _BaseCrops(
        csv_fp=Path(args.test_root) / "test_labels.csv",
        img_dir=Path(args.test_root) / "merged_images",
        img_size=(256, 64),
        for_train=False,
    )
    test_set.preload_for_stats()
    test_set.print_filter_report()

    # â‘¤ DataLoader ìƒì„±
    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_train,
        drop_last=True,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_subset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        collate_fn=collate_train,  # validationë„ loss ê³„ì‚°ì„ ìœ„í•´ train collate ì‚¬ìš©
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_set,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        collate_fn=collate_eval,
        pin_memory=True
    )

    # â‘¥ finetuning ì‹œì‘ (validation ì‚¬ìš©)
    finetune(
        recognizer=rec,
        op2val=opt_dict,
        converter=converter,
        train_loader=train_loader,
        valid_loader=val_loader,  # validation loader ì‚¬ìš©
        test_loader=test_loader,
        epochs=epochs,
        lr=lr,
        save_dir=Path(save_dir),
        device=args.device
    )
    
    save_ckpt(rec, opt_dict, save_dir)

    # â‘¦ í›ˆë ¨ ì™„ë£Œ í›„ ìµœì¢… í…ŒìŠ¤íŠ¸ (best ëª¨ë¸ ë¡œë“œ)
    print("\n" + "="*50)
    print("ìµœì¢… í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("="*50)
    
    final_reader = brainocr.Reader(
        lang="ko",
        det_model_ckpt_fp="/workspace/SKKUOCR2/SKKUOCR_fine/assets/craft.pt",
        rec_model_ckpt_fp=str(Path(save_dir) / "best.pt"),
        opt_fp=str(Path(save_dir) / "finetune_clova_opt.txt"),
        device=args.device,
    )
    final_reader.recognizer.to(args.device)

    # ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê³„ì‚°
    test_acc, test_loss, test_sum_counts, test_avg_confs  = evaluate_accuracy(final_reader.recognizer, opt_dict, converter, test_loader, device=args.device)
    print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼: accuracy={test_acc*100:.2f}%")
    
    # WandBì— ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
    wandb.log({
        "final_test_accuracy": test_acc,
        "final_test_loss": test_loss
    })

    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ì €ì¥
    evaluate_dataset(final_reader, test_loader, device=args.device,
                     save_csv=str(Path(save_dir) / "test_pred.csv"))

    wandb.finish()
    print(f"âœ… ëª¨ë“  ê³¼ì • ì™„ë£Œ! ê²°ê³¼ëŠ” {save_dir}ì— ì €ì¥ë¨")


def test_only(model_ckpt_fp, opt_fp, test_root, device="cuda"):
    # Reader ì´ˆê¸°í™”
    reader = brainocr.Reader(
        lang="ko",
        det_model_ckpt_fp="/workspace/SKKUOCR2/SKKUOCR_fine/assets/craft.pt",
        rec_model_ckpt_fp=str(model_ckpt_fp),
        opt_fp=str(opt_fp),
        device=device,
    )
    reader.recognizer.to(device)

    # í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ
    test_set = _BaseCrops(
        csv_fp=Path(test_root) / "test_labels.csv",
        img_dir=Path(test_root) / "merged_images",
        img_size=(256, 64),
        for_train=False,
    )
    test_set.preload_for_stats()
    test_set.print_filter_report()

    test_loader = DataLoader(
        test_set, batch_size=64, shuffle=False,
        num_workers=2, collate_fn=collate_eval, pin_memory=True
    )

    # í‰ê°€ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
    evaluate_dataset(reader, test_loader, device=device, save_csv="assets/test_pred.csv")

    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ê²°ê³¼ëŠ” assets/test_pred.csvì— ì €ì¥ë¨.")


# --------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--opt_txt",   default="ocr-opt.txt")
    parser.add_argument("--train_root", default="CLOVA_V3_train")
    parser.add_argument("--test_root",  default="CLOVA_V2_test")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--batch", type=int, default=64)
    parser.add_argument("--device", default="cuda")
    parser.add_argument("--save_dir", default="assets")
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--val_ratio", type=float, default=0.2, help="Validation split ratio (default: 0.2)")
    args = parser.parse_args()
    
    train_and_evaluate(
        epochs=args.epochs,
        batch_size=args.batch,
        lr=args.lr,
        args=args
    )

    # ë§Œì•½ ì˜¤ë¡œì§€ í…ŒìŠ¤íŠ¸ë§Œ ëŒë¦¬ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
    # test_only(
    #     model_ckpt_fp="assets/test_20/finetune_clova.pt",
    #     opt_fp="assets/test_20/finetune_clova_opt.txt",
    #     test_root="CLOVA_V2_test",
    #     device="cuda"
    # )